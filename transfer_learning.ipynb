{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, Features\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Depending on the emotion, an appropriate measu...</td>\n",
       "      <td>Purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We apply this principle to gigapixel image ren...</td>\n",
       "      <td>Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this study, a random forest model with two ...</td>\n",
       "      <td>Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We present a framework for acuity-driven visua...</td>\n",
       "      <td>Purpose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For this purpose, a procedure for characterizi...</td>\n",
       "      <td>Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>At last, we discuss several trends in auto-par...</td>\n",
       "      <td>Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Using graphical demonstration, the impact of v...</td>\n",
       "      <td>Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>It can not only enable the clients to preserve...</td>\n",
       "      <td>Background of Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Results showed that the novel Hybrid RuDSTCs c...</td>\n",
       "      <td>Results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>To handle the above problem, researchers propo...</td>\n",
       "      <td>Purpose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                   label\n",
       "0    Depending on the emotion, an appropriate measu...                 Purpose\n",
       "1    We apply this principle to gigapixel image ren...                 Methods\n",
       "2    In this study, a random forest model with two ...                 Methods\n",
       "3    We present a framework for acuity-driven visua...                 Purpose\n",
       "4    For this purpose, a procedure for characterizi...                 Methods\n",
       "..                                                 ...                     ...\n",
       "246  At last, we discuss several trends in auto-par...                 Methods\n",
       "247  Using graphical demonstration, the impact of v...                 Methods\n",
       "248  It can not only enable the clients to preserve...  Background of Research\n",
       "249  Results showed that the novel Hybrid RuDSTCs c...                 Results\n",
       "250  To handle the above problem, researchers propo...                 Purpose\n",
       "\n",
       "[251 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Dataset.xlsx\")\n",
    "data = data.rename(columns={'Sentence': 'text', 'Label': 'label'})\n",
    "data = data.sample(frac = 1).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 384)\n",
      "(251,)\n"
     ]
    }
   ],
   "source": [
    "sbert = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "X = sbert.encode(data['Sentence'])\n",
    "y = data['Label'].replace({'a1': 0, 'a2': 1, 'a3' : 2, 'a4' : 3, 'a5' : 4}, inplace=False).to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1024, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(256, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dense(32, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 2s 32ms/step - loss: 0.1283 - accuracy: 0.9771\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.5125 - accuracy: 0.9543\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 0.1263 - accuracy: 0.9657\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1204 - accuracy: 0.9714\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.1744 - accuracy: 0.9714\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.0819 - accuracy: 0.9771\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0388 - accuracy: 0.9886\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.0478 - accuracy: 0.9829\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.0295 - accuracy: 0.9829\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.1436 - accuracy: 0.9714\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0800 - accuracy: 0.9771\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.0546 - accuracy: 0.9771\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 0.0214 - accuracy: 0.9886\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0304 - accuracy: 0.9829\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 0.0405 - accuracy: 0.9886\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0255 - accuracy: 0.9829\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 0.0240 - accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0210 - accuracy: 0.9829\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0264 - accuracy: 0.9829\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.0334 - accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs = 20, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.25      0.26        12\n",
      "           1       0.40      0.40      0.40        10\n",
      "           2       0.59      0.62      0.60        26\n",
      "           3       0.46      0.43      0.44        14\n",
      "           4       0.33      0.36      0.34        14\n",
      "\n",
      "    accuracy                           0.45        76\n",
      "   macro avg       0.41      0.41      0.41        76\n",
      "weighted avg       0.44      0.45      0.45        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)\n",
    "print(classification_report(y_pred_class, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 201\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = Features.from_dict({'text': {'dtype': 'string', 'id': None, '_type': 'Value'}, 'label': {'names': data['label'].unique().tolist(), 'id': None, '_type': 'ClassLabel'}})\n",
    "\n",
    "train = data.groupby('label').head(10).reset_index(drop=True)\n",
    "test = data.groupby('label').tail(-10).reset_index(drop=True)\n",
    "train = Dataset.from_pandas(train, features=features)\n",
    "test = Dataset.from_pandas(test, features=features)\n",
    "print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 466.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 909.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "train = train.map(preprocess_function)\n",
    "test = test.map(preprocess_function)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "id2label = {i: label for i, label in enumerate(data['label'].unique().tolist())}\n",
    "label2id = {label: i for i, label in enumerate(data['label'].unique().tolist())}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|â–ˆ         | 4/40 [00:20<02:31,  4.22s/it]\n",
      " 10%|â–ˆ         | 4/40 [00:45<02:31,  4.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6099519729614258, 'eval_accuracy': 0.17412935323383086, 'eval_runtime': 23.3425, 'eval_samples_per_second': 8.611, 'eval_steps_per_second': 0.557, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 8/40 [01:11<03:58,  7.44s/it]\n",
      " 20%|â–ˆâ–ˆ        | 8/40 [01:34<03:58,  7.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6069296598434448, 'eval_accuracy': 0.18407960199004975, 'eval_runtime': 22.9251, 'eval_samples_per_second': 8.768, 'eval_steps_per_second': 0.567, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [02:01<03:38,  7.79s/it]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [02:23<03:38,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6029309034347534, 'eval_accuracy': 0.19900497512437812, 'eval_runtime': 22.1417, 'eval_samples_per_second': 9.078, 'eval_steps_per_second': 0.587, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [02:48<03:02,  7.62s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [03:09<03:02,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.59913170337677, 'eval_accuracy': 0.21890547263681592, 'eval_runtime': 21.0562, 'eval_samples_per_second': 9.546, 'eval_steps_per_second': 0.617, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [03:32<02:34,  7.72s/it]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [03:54<02:34,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5963943004608154, 'eval_accuracy': 0.21393034825870647, 'eval_runtime': 21.3136, 'eval_samples_per_second': 9.431, 'eval_steps_per_second': 0.61, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [04:19<02:07,  7.94s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [04:41<02:07,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5941962003707886, 'eval_accuracy': 0.208955223880597, 'eval_runtime': 21.7113, 'eval_samples_per_second': 9.258, 'eval_steps_per_second': 0.599, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [05:02<01:28,  7.41s/it]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [05:24<01:28,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5925624370574951, 'eval_accuracy': 0.21393034825870647, 'eval_runtime': 22.1565, 'eval_samples_per_second': 9.072, 'eval_steps_per_second': 0.587, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [05:48<01:00,  7.56s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [06:09<01:00,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5917268991470337, 'eval_accuracy': 0.20398009950248755, 'eval_runtime': 20.7906, 'eval_samples_per_second': 9.668, 'eval_steps_per_second': 0.625, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [06:30<00:28,  7.03s/it]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [06:52<00:28,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5911369323730469, 'eval_accuracy': 0.19402985074626866, 'eval_runtime': 21.8503, 'eval_samples_per_second': 9.199, 'eval_steps_per_second': 0.595, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:17<00:00,  7.60s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:38<00:00,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5909351110458374, 'eval_accuracy': 0.19402985074626866, 'eval_runtime': 21.2672, 'eval_samples_per_second': 9.451, 'eval_steps_per_second': 0.611, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:46<00:00, 11.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 466.9598, 'train_samples_per_second': 1.071, 'train_steps_per_second': 0.086, 'train_loss': 1.5626529693603515, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:20<00:00,  1.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5909351110458374,\n",
       " 'eval_accuracy': 0.19402985074626866,\n",
       " 'eval_runtime': 21.4278,\n",
       " 'eval_samples_per_second': 9.38,\n",
       " 'eval_steps_per_second': 0.607,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=5, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 47.8kB/s]\n",
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhair\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629/629 [00:00<00:00, 571kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 536kB/s]\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:14<00:00, 18.0MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 430.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 860.23 examples/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|â–ˆ         | 4/40 [00:21<02:40,  4.45s/it]\n",
      " 10%|â–ˆ         | 4/40 [00:45<02:40,  4.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6149466037750244, 'eval_accuracy': 0.17412935323383086, 'eval_runtime': 23.3535, 'eval_samples_per_second': 8.607, 'eval_steps_per_second': 0.557, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 8/40 [01:09<03:41,  6.92s/it]\n",
      " 20%|â–ˆâ–ˆ        | 8/40 [01:28<03:41,  6.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6122069358825684, 'eval_accuracy': 0.17412935323383086, 'eval_runtime': 19.6944, 'eval_samples_per_second': 10.206, 'eval_steps_per_second': 0.66, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [01:53<03:18,  7.09s/it]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [02:15<03:18,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6085256338119507, 'eval_accuracy': 0.208955223880597, 'eval_runtime': 22.0558, 'eval_samples_per_second': 9.113, 'eval_steps_per_second': 0.589, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [02:40<02:58,  7.43s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [03:02<02:58,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6058290004730225, 'eval_accuracy': 0.22885572139303484, 'eval_runtime': 21.8818, 'eval_samples_per_second': 9.186, 'eval_steps_per_second': 0.594, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [03:26<02:37,  7.86s/it]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [03:48<02:37,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6042907238006592, 'eval_accuracy': 0.22885572139303484, 'eval_runtime': 21.5405, 'eval_samples_per_second': 9.331, 'eval_steps_per_second': 0.604, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [04:14<02:12,  8.30s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [04:38<02:12,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6031250953674316, 'eval_accuracy': 0.22885572139303484, 'eval_runtime': 23.1842, 'eval_samples_per_second': 8.67, 'eval_steps_per_second': 0.561, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [05:00<01:33,  7.78s/it]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [05:23<01:33,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6026393175125122, 'eval_accuracy': 0.22388059701492538, 'eval_runtime': 22.6415, 'eval_samples_per_second': 8.878, 'eval_steps_per_second': 0.574, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [05:47<01:01,  7.74s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [06:09<01:01,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6011980772018433, 'eval_accuracy': 0.22388059701492538, 'eval_runtime': 21.8256, 'eval_samples_per_second': 9.209, 'eval_steps_per_second': 0.596, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [06:31<00:29,  7.41s/it]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [06:53<00:29,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6003878116607666, 'eval_accuracy': 0.23880597014925373, 'eval_runtime': 21.5266, 'eval_samples_per_second': 9.337, 'eval_steps_per_second': 0.604, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:17<00:00,  7.59s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:39<00:00,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6000033617019653, 'eval_accuracy': 0.24875621890547264, 'eval_runtime': 21.6838, 'eval_samples_per_second': 9.27, 'eval_steps_per_second': 0.6, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [07:47<00:00, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 468.1127, 'train_samples_per_second': 1.068, 'train_steps_per_second': 0.085, 'train_loss': 1.5118670463562012, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.5118670463562012, metrics={'train_runtime': 468.1127, 'train_samples_per_second': 1.068, 'train_steps_per_second': 0.085, 'train_loss': 1.5118670463562012, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer2)\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=5, id2label = id2label, label2id = label2id, ignore_mismatched_sizes=True\n",
    "\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer2(examples[\"text\"], truncation=True)\n",
    "\n",
    "train1 = train.map(preprocess_function)\n",
    "test1 = test.map(preprocess_function)\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    train_dataset=train1,\n",
    "    eval_dataset=test1,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 405.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 916.63 examples/s]\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Changing param values is not allowed. Param with key='torch_dtype' was already logged with value='None' for run ID='4187cd981e3d49f0b589f5f4663d93b9'. Attempted logging new value 'float16'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:1048\u001b[0m, in \u001b[0;36mFileStore.log_batch\u001b[1;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m-> 1048\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_run_param(run_info, param)\n\u001b[0;32m   1049\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:953\u001b[0m, in \u001b[0;36mFileStore._log_run_param\u001b[1;34m(self, run_info, param)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(param_path):\n\u001b[1;32m--> 953\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_new_param_value(\n\u001b[0;32m    954\u001b[0m         param_path\u001b[39m=\u001b[39;49mparam_path,\n\u001b[0;32m    955\u001b[0m         param_key\u001b[39m=\u001b[39;49mparam\u001b[39m.\u001b[39;49mkey,\n\u001b[0;32m    956\u001b[0m         run_id\u001b[39m=\u001b[39;49mrun_info\u001b[39m.\u001b[39;49mrun_id,\n\u001b[0;32m    957\u001b[0m         new_value\u001b[39m=\u001b[39;49mwriteable_param_value,\n\u001b[0;32m    958\u001b[0m     )\n\u001b[0;32m    959\u001b[0m make_containing_dirs(param_path)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:973\u001b[0m, in \u001b[0;36mFileStore._validate_new_param_value\u001b[1;34m(self, param_path, param_key, run_id, new_value)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m current_value \u001b[39m!=\u001b[39m new_value:\n\u001b[1;32m--> 973\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[0;32m    974\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChanging param values is not allowed. Param with key=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparam_key\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m was already\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    975\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m logged with value=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcurrent_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for run ID=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrun_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Attempted logging\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    976\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m new value \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mnew_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    977\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[0;32m    978\u001b[0m     )\n",
      "\u001b[1;31mMlflowException\u001b[0m: Changing param values is not allowed. Param with key='torch_dtype' was already logged with value='None' for run ID='4187cd981e3d49f0b589f5f4663d93b9'. Attempted logging new value 'float16'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Project (NLP Articles)\\transfer_learning.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test2 \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mmap(preprocess_function)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer3 \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel4,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project%20%28NLP%20Articles%29/transfer_learning.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m trainer3\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1826\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step\n\u001b[0;32m   1824\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m-> 1826\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[0;32m   1828\u001b[0m \u001b[39m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mignore_data_skip:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\trainer_callback.py:362\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m    361\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\trainer_callback.py:406\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    405\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 406\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[0;32m    407\u001b[0m             args,\n\u001b[0;32m    408\u001b[0m             state,\n\u001b[0;32m    409\u001b[0m             control,\n\u001b[0;32m    410\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    411\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[0;32m    412\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[0;32m    413\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[0;32m    414\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[0;32m    415\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[0;32m    416\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    417\u001b[0m         )\n\u001b[0;32m    418\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:1023\u001b[0m, in \u001b[0;36mMLflowCallback.on_train_begin\u001b[1;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args, state, control, model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1022\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[1;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:1014\u001b[0m, in \u001b[0;36mMLflowCallback.setup\u001b[1;34m(self, args, state, model)\u001b[0m\n\u001b[0;32m   1012\u001b[0m combined_dict_items \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(combined_dict\u001b[39m.\u001b[39mitems())\n\u001b[0;32m   1013\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(combined_dict_items), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH):\n\u001b[1;32m-> 1014\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ml_flow\u001b[39m.\u001b[39;49mlog_params(\u001b[39mdict\u001b[39;49m(combined_dict_items[i : i \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_MAX_PARAMS_TAGS_PER_BATCH]))\n\u001b[0;32m   1015\u001b[0m mlflow_tags \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mMLFLOW_TAGS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m mlflow_tags:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:755\u001b[0m, in \u001b[0;36mlog_params\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    753\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[0;32m    754\u001b[0m params_arr \u001b[39m=\u001b[39m [Param(key, \u001b[39mstr\u001b[39m(value)) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m--> 755\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mlog_batch(run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49m[], params\u001b[39m=\u001b[39;49mparams_arr, tags\u001b[39m=\u001b[39;49m[])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\tracking\\client.py:1038\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[1;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_batch\u001b[39m(\n\u001b[0;32m    980\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    981\u001b[0m     run_id: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m     tags: Sequence[RunTag] \u001b[39m=\u001b[39m (),\n\u001b[0;32m    985\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[39m    Log multiple metrics, params, and/or tags.\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[39m        status: FINISHED\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_batch(run_id, metrics, params, tags)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:389\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[1;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[0;32m    386\u001b[0m     metrics_batch \u001b[39m=\u001b[39m metrics[:metrics_batch_size]\n\u001b[0;32m    387\u001b[0m     metrics \u001b[39m=\u001b[39m metrics[metrics_batch_size:]\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[0;32m    390\u001b[0m         run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49mmetrics_batch, params\u001b[39m=\u001b[39;49mparams_batch, tags\u001b[39m=\u001b[39;49mtags_batch\n\u001b[0;32m    391\u001b[0m     )\n\u001b[0;32m    393\u001b[0m \u001b[39mfor\u001b[39;00m metrics_batch \u001b[39min\u001b[39;00m chunk_list(metrics, chunk_size\u001b[39m=\u001b[39mMAX_METRICS_PER_BATCH):\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mlog_batch(run_id\u001b[39m=\u001b[39mrun_id, metrics\u001b[39m=\u001b[39mmetrics_batch, params\u001b[39m=\u001b[39m[], tags\u001b[39m=\u001b[39m[])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py:1059\u001b[0m, in \u001b[0;36mFileStore.log_batch\u001b[1;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_run_tag(run_info, tag)\n\u001b[0;32m   1058\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1059\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(e, INTERNAL_ERROR)\n",
      "\u001b[1;31mMlflowException\u001b[0m: Changing param values is not allowed. Param with key='torch_dtype' was already logged with value='None' for run ID='4187cd981e3d49f0b589f5f4663d93b9'. Attempted logging new value 'float16'."
     ]
    }
   ],
   "source": [
    "tokenizer3 = AutoTokenizer.from_pretrained(\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "data_collator3 = DataCollatorWithPadding(tokenizer=tokenizer3)\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\", num_labels=5, id2label = id2label, label2id = label2id, ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer3(examples[\"text\"], truncation=True)\n",
    "\n",
    "train2 = train.map(preprocess_function)\n",
    "test2 = test.map(preprocess_function)\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args,\n",
    "    train_dataset=train2,\n",
    "    eval_dataset=test2,\n",
    "    tokenizer=tokenizer3,\n",
    "    data_collator=data_collator3,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer3.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
